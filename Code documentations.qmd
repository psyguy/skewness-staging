---
title: "Skewness and staging: Does the floor effect induce bias in multilevel AR(1) models?"
subtitle: "Reproducible codes"
author: "MH Manuel Haqiqatkhah"
format:
  html:
    code-fold: true
    code-tools: true
    code-overflow: scroll
    output-file: "index.html"
    code-line-numbers: true
    toc: true
    toc-location: left
    toc-depth: 5
    self-contained: false
    code-summary: "Click to expand the code"
    anchor-sections: true
    reference-location: document
    citation-location: margin
  # theme:
  #   light: litera
  #   dark: darkly
  pdf:
    number-sections: true
    number-depth: 8
    toc: true
    output-file: Code documentation
    toc-depth: 8
## For html
execute:
  enabled: true
  echo: true
  include: true
  warning: false
  error: false
  freeze: auto
  cache: refresh
## For pdf
# execute:
#   enabled: true
#   echo: false
#   include: false
#   warning: false
#   error: false
#   freeze: auto
#   cache: refresh
editor: visual
bibliography: references.bib
---

# Introduction {#sec-introduction}

This document contains the reproducible code for the manuscript Skewness and staging: Does the floor effect induce bias in multilevel AR(1) models? by M. M. Haqiqatkhah, O. Ryan, and E. L. Hamaker. please cite as:

...

In this study, we simulated multilevel data from three data generating mechanisms (DGMs), namely, the AR(1, $\chi^2$AR(1), BinAR(1), and PoDAR(1) models with different parameter sets.\
For details, see the paper.

The simulation was conducted using the following modular pipeline design, inspired by Bien's R package `simulator` [-@bien_2016_SimulatorEngineStreamline], consisting of the following **components**:

A.  *Simulation*: generating the datasets
B.  *Analysis*: modeling the data
C.  *Harvesting*: collecting the relevant parameter estimates
D.  *Reporting*: making tables and plots

And the components were placed in a **pipeline**, that managed:

1.  Making the simulation design matrix that include all relevant conditions
2.  Book-keeping data files belonging to each replication of each condition
3.  Performing simulations in batch
4.  Performing Analyses in batch
5.  Collecting the data in batch

This document is structured as follows. In @sec-components, we explain the four components and the functions used therein. Then, in @sec-pipeline we explain the wrapper functions used in the pipeline, and show how the pipeline was---and can be---executed. Then, in @sec-simulation-results, we discuss how the harvested data was used to make the figures used in the paper (and others that were not included). Finally, in @sec-additional-analyses-and-figures we discuss how supplementary plots were made and how the empirical data analysis (on COGITO data) was done.

<!--# In what follows we describe the steps and provide the codes used in this study. In the other tab of this document, we provide the code to generate additional plots shown in the paper. Note that although the codes provided here are cleaned as much as possible, they are not necessarily succinctly written; some functions were written to accommodate the most general functionalities which turned out to be not necessary for the simulation study. -->

Before we begin, we need to read the scripts to include them in this document. By default, none of the scripts run here (as they are time consuming). To run the scripts of each component, you can change the following variables to `TRUE` and re-render the document:

```{r}
#| eval: true
#| code-fold: false
runComponent_Simulation <- FALSE
runComponent_Analysis <- FALSE
runComponent_Harvesting <- FALSE
runComponent_Reporting <- FALSE
```

In case you want to change the scripts (e.g., to run a smaller portion of the simulation, or try other parameters, etc.), you should look up the `kintr` parameters called in each chunk (with `<<some_param>>`) and find the corresponding code (under `## @knitr some_param`) in the `scripts` folder.

```{r}
#| label: setup
#| echo: false
#| include: false
#| eval: true
#| cache: false
#| freeze: false

scripts <- list.files(path = "scripts/",
                      # pattern = ".r|.R",
                      full.names = TRUE)

sapply(scripts, knitr::read_chunk)

runComponent_Initialization <- runComponent_Simulation | runComponent_Analysis | runComponent_Harvesting | runComponent_Reporting

library(knitr)

```

```{r, eval = runComponent_Initialization}
#| include: false
<<load_packages>>
```

# Core components {#sec-components}

## The *Simulation* component {#sec-simulation-component}

The Simulation component consists of three sets of functions:

i.  Functions that implement the DGMs and generate univariate ($N=1$) time series of length $T$ from the parameters given to them;
ii. Wrappers that interface the DGM functions;
iii. A wrapper to generate datasets (consisting on $N$ time series of length $T$) with a given DGM

### Data-generating models specifications {#sec-dgm-specifications}

First we define functions for each data-generating models (DGMs) that can produce univariate, single-subject ($N=1$) time series of desired length $T$ (default: `T = 100`) with the two canonical parameters and a given random seed (default: `seed = 0`). All model(-implied) parameters are saved in a list (called `pa`).

For each model, the first observation ($X_1$) is randomly drawn from the model-implied marginal distribution, to eliminate the need for removing the burn-in window in the beginning of the data. After the data is generated, in case the argument `only.ts` is set to be `TRUE`, the raw data (as a vector of length `T`) is returned. Otherwise, the function calculates empirical dynamic ($\phi$) and marginal ($\mu$, $\sigma^2$, and $\gamma$) parameters based on the simulated data, and save it in a list (`Empirical.Parameters`). Furthermore, two `r ifelse(knitr::is_html_output(), r"($\mathrm{\LaTeX{}}$)", r"(\LaTeX{})")`-ready strings (`Model.Description` and `Model.Description.Short`) are made which include a summary of the model parameters (that can be used, e.g., in plots). Finally, in case `only.ts != TRUE`, the function returns a list consisting of the time series (stored in `x`), verbal description of the dataset (`Model.Description` and `Model.Description.Short`), theoretical (i.e., model-implied) parameters (`Model.Parameters`), and empirical (i.e., sample) estimated parameters (`Empirical.Parameters`).

::: panel-tabset
#### The AR(1) model

The canonical parameters of the AR(1) model with normally distributed residuals (which we referred to as `NAR(1)` in the simulation) are the autoregressive parameter $\phi$ (default: `phi = 0.4`), mean $\mu$ (default: `Mean = 50`), and the marginal variance $\sigma^2$ (default: `var.marginal = 4`). Based on the marginal variance, the residual variance (`var.resid`) is calculated via $\sigma^2_\epsilon = \sigma^2 (1 - \phi^2)$.

##### Construction

The time series is constructed by first generating a zero-centered time series $\tilde X_t$ (`x_cent`). To do so, first the initial observation in the time series (`x_cent[1]`) is sampled from normal distribution with mean zero and a variance equal to the marginal variance of the model:

$$
\tilde X_1 \sim \mathcal{N}(0, \sigma^2)
$$

Then, the remainder of the time series is generated using the definition of the AR(1) model (not that the here the residual variance is used in the normal distribution):

$$
\begin{aligned}
\tilde X_{t} &= \phi \tilde X_{t-1} + \epsilon_{t} \\
\epsilon_{t} &\sim \mathcal{N}(0, {\sigma^2_{\epsilon}})
\end{aligned}
$$

Finally, the mean is added to the centered zero-centered time series to reach the final time series with mean $\mu$:

$$
X_t = \tilde X_t + \mu
$$

##### Code

```{r, eval = runComponent_Simulation}
<<dgm_nar>>
```

#### The $\chi^2$AR(1) model

The canonical parameters of the $\chi^2$AR(1) model (which we referred to as `ChiAR(1)` in the simulation) are the autoregressive parameter $\phi$ (default: `phi = 0.4`), and degrees of freedom $\nu$ (default: `nu = 3`). We set the intercept to zero (`c = 0`).[^1]

##### Construction

Similar to the AR(1) model, we need to sample the first observation of the $\chi^2$AR(1) model from its marginal distribution. However, since this model does not have a closed-form marginal distribution, as an approximation, we instead sample `x[1]` from a $\chi^2$ distribution with $\nu$ degrees of freedom:

$$
X_1 \sim \chi^2(\nu)
$$

Then, we generate the remainder of the time series using the definition of the $\chi^2$AR(1) model:

$$
\begin{aligned}
X_{t} &= c + \phi X_{t-1} + a_{t} \\
a_{t} &\sim \chi^2(\nu).
\end{aligned}
$$

##### Code

```{r, eval = runComponent_Simulation}
<<dgm_chiar>>
```

#### The BinAR(1) model

The canonical parameters of the BinAR(1) model (which we referred to as `BinAR(1)` in the simulation) are the survival probability $\alpha$ (default: `alpha = 0.5`) and the revival probability $\beta$ (default: `beta = 0.4`). By default, the maximum value on scale $k$ was set to `k = 10`.

##### Construction

We first calculate the $\theta$ parameter, which characterizes the marginal distribution of the BinaR(1) model:

$$
\theta = \frac{k \beta}{1-(\alpha-\beta)}
$$ Then we draw $X_1$ (`x[1]`) from the marginal distribution of the model:

$$
X_1 \sim Binom(k, \theta)
$$

The rest of time series is generated sequentially, for each time point $t$, by drawing values for the number of survived (`S_t[t]`) and revived (`R_t[t]`) elements of the BinAR(1) model based on the previous observations ($X_{t-1}$), and then adding them:

$$
\begin{aligned}
S_{t} &\sim Binom(X_{t-1}, \alpha) \\
R_t &\sim Binom(k -X_{t-1}, \beta) \\
X_{t} &= S_t + R_t
\end{aligned}
$$

##### Code

```{r, eval = runComponent_Simulation}
<<dgm_binar>>
```

#### The PoDAR(1) model

The canonical parameters of the PoDAR(1) model (which we referred to as `PoDAR(1)` in the simulation) are the persistence probability $\tau$ (default: `tau = 0.7`) and the average rate $\lambda$ (default: `lambda = 0.5`).

##### Construction

To generate the time series, we first draw the first observation $X_1$ (`x[1]`) from a Poisson distribution with rate $\lambda$:

$$
X_1 \sim Poisson(\lambda)
$$

And generate the rest of the time series by first drawing $Z_t$ from a Poisson distribution with rate $\lambda$ and $P_t$ from a binomial distribution with size probability of success $\tau$ (that is equivalent to a Bernoulli distribution with probability $\tau$). Then, we calculate $X_t$ based on the previous observation (`x[t-1]`) and values of $Z_t$ (`Z_t[t]`) and $P_t$ (`P_t[t]`), using the definition of the PoDAR(1) model:

$$
\begin{aligned}
Z_t &\sim Poisson(\lambda) \\
P_t &\sim Binom(1, \tau) \\
X_t &= P_t X_{t-1} + (1-P_t) Z_t
\end{aligned}
$$

##### Code

```{r, eval = runComponent_Simulation}
<<dgm_podar>>
```
:::

[^1]: The $\chi^2$AR(1), in a more general form, can have an intercept ($X_{t} = c + \phi X_{t-1} + a_t, \quad a_t \sim \chi^2(\nu)$. Since the intercept was set to zero in the simulation study, we discussed a zero-intercept version of this model ($c=0$) in the paper. See the Supplemental Materials for more details.

### General DGM wrappers {#sec-general-dgm-wrappers}

Given that, in each model, two canonical parameters characterize the dynamic and marginal features of the generated time series, and given that we have analytic formulas that link the canonical parameters to the model-implied $\phi$, $\mu$, $\sigma^2$, and $\gamma$, we use a function (`dgm_parameterizer`) to calculate canonical parameters from two given parameters, and make a complete list of parameters (called `pa`). This list also includes non-parameter variables, importantly, the time series length $T$ (saved in `pa$T`) and the random seed used in the `dgm_*` functions (saved in `pa$seed`). A wrapper function (`dgm_generator`) is used as an interface to all `dgm_*` functions, which first makes sure the given parameters are sufficient for data generation, makes a complete parameter list `pa` with the help of `dgm_parameterizer`, and passes `pa` to the respective DGM generating function.

::: panel-tabset
#### Parameter conversions

The function `dgm_parameterizer` calculates canonical/model-implied parameters of a given DGM (specified using the `Model` argument) based on the parameters given to it as arguments, and saves them in a list of parameters (`pa`), which s returned by the function. The function makes sure that the set of parameters provided are sufficient to characterize the dynamic parameter of the model (i.e., the autoregression $\phi$) and at least one of the marginal parameters (importantly, the mean $\mu$) but giving default values to some parameters.

```{r, eval = runComponent_Simulation}
<<dgm_parameterizer>>
```

#### Wrapper around `dgm_*` functions

The function `dgm_generator` gets a set of parameters (either as separate arguments, or a list of parameters, like the one returned by `dgm_parameterizer`), saves them in a list called `pa`. It checks whether $\phi$ is included in the list (if not, sets the default value `pa$phi = 0.2`), and checks if at least one other parameter (which, together with $\phi$, is required to characterize the marginal properties of the DGMs) is calculated for it (if not, it sets the default value `pa$Mean = 5` for $\mu$). Furthermore, if the DGM name, time series length, and the random seed are not provided, it gives them default values (respectively: `Model = "ChiAR(1)"`, `T = 100`, and `seed = 0`) and adds them to `pa`.

Then, it passes the `pa` list to `dgm_parameterizer` to do the necessary conversions to complete the list of canonical and model-implied parameters. Finally, given the model name, it checks if non-canonical parameters $k$ and $c$ are set (otherwise assigns appropriate defaults to them), and passes the complete parameter list to the respective DGM function.

```{r, eval = runComponent_Simulation}
<<dgm_generator>>
```
:::

### Dataset generation {#sec-dataset-generation}

The machinery described above can be used to generate individual ($N=1$) time series. However, for the simulation study, we need datasets comprising of multiple ($N=25, 50, 100$) individuals. As we discussed in the paper, in our study, all individuals in a dataset of a DGM share the same autoregressive parameter ($\phi_i=0.4$) and the individual differences are only in the individual means ($\mathbb{\mu} = [\mu_1 , \mu_2, \dots, \mu_N]$). Thus, we write a function (`dgm_make.sample`) that can generate, for each DGM, a dataset of $N$ individuals based on an $N$-dimensional vector of individual means, all with the same $\phi_i$. We then need to find the appropriate parameters for the level-2 distributions (Gaussian and $\chi^2$ distributions) for each DGM, such that the we get a considerable proportion of individuals with considerably high skewness while respecting the lower and upper bounds of values supported by each model. Finally, with a wrapper function (`make_datasets`), we facilitate making dataset by automatically generating the means vector suitable for each DGM.

::: panel-tabset
#### Making individual datasets

The function `dgm_make.sample` generates a dataset of time series of length `T` with the autoregressive parameter `phi` from a desired DGM (determined by the `Model` argument) given a vector of means (passed as the argument `Means`). The length of `Means` determine the number of individuals in the dataset (`N <- length(Means)`). If `Means` is not provided, a randomly generated vector of $N = 100$ is used as default. Since each individual time series is generated with a random seed, we need a vector of `N` unique seeds, which can be provided using the `seeds` argument. In case `seeds` is not provided, it is generated based on the provided means (`seeds.from.means`), and if it is a scalar, the seeds vector is created by adding the scalar to the `seeds.from.means` (which would allow generating different datasets with the same mean distributions).

```{r, eval = runComponent_Simulation}
<<dgm_make.sample>>
```

#### Determining level-2 distribution parameters

For each alternative DGM---the $\chi^2$AR(1), BinAR(1), and PoDAR(1) models---we should determine appropriate parameters for the level-2 distribution of means such that we have enough skewness in the generated datasets. To do so, we make a function (`Mean.vs.Skewness`) to help us experiment with different values for $\mu$ and $\sigma^2$ (of the Gaussian level-2 distribution) and $\nu$ (of the $\chi^2$ level-2 distribution) for each alternative DGM. Note that we start by generating more than enough samples for each distribution ($10 \times N$) and subsample $N$ values after applying the model-specific lower and upper bounds.

```{r}
#| eval: true
#| echo: false
#| include: false
<<Mean.vs.Skewness>>
```

We notice that we get the desired distribution of skewness with the following parameters:

| Model         | $\mu$ | $\sigma^2$ | $\nu$ |
|---------------|-------|------------|-------|
| $\chi^2$AR(1) | 10    | 10         | 5     |
| BinAR(1)      | 2     | 1          | 2.9   |
| PoDAR(1)      | 4     | 4          | 1.5   |

: Parameters of level-2 distribution of means

Giving us the following distributions:

```{r}
#| eval: false
#| echo: false
#| include: false


gaussian.means <-
  wrap_elements(Mean.vs.Skewness("Chi2AR",
     l2.mean = 10,
     l2.var = 10,
     seed = 1)) /
  wrap_elements(Mean.vs.Skewness("BinAR",
     l2.mean = 2,
     l2.var = 1,
     seed = 4)) /
  wrap_elements(Mean.vs.Skewness("PoDAR",
                   l2.mean = 4,
     seed = 2))

chi2.means <-
  wrap_elements(Mean.vs.Skewness("Chi2AR",
     chi2.df = 5,
     seed = 1)) /
  wrap_elements(Mean.vs.Skewness("BinAR",
     chi2.df = 2.9,
     seed = 1)) /
  wrap_elements(Mean.vs.Skewness("PoDAR",
                   chi2.df = 1.5,
     seed = 4))

ggsave("Mean_vs_Skewness.png",
       wrap_elements(gaussian.means) +
       wrap_elements(chi2.means),
       path = "additional-files",
       width = 40,
       height = 60,
       units = "cm")
```

![](additional-files/Mean_vs_Skewness.png)

#### Automate dataset generation

We then use a wrapper (`make_datasets`) around `dgm_make.sample` that generates datasets for all four DGMs with the appropriate level-2 parameters specified above. Note that here we first generate more than enough (i.e., $2 \times N$) samples of means to make sure we end up with $N$ samples after applying the upper and lower bounds. The datasets are then saved, with some additional variables in separate `.rds` files, using wrapper functions described in @sec-pipeline-functions.

```{r, eval = runComponent_Simulation}
<<make_datasets>>
```
:::

## The *Analysis* component {#sec-analysis-component}

We analyzed each dataset with AR(1) models with fixed and random residual variance using *M*plus v. 8.1 [@muthen_2017_MplusUserGuide]. To interface *M*plus from R, we used the package `MplusAutomation` [@hallquist_2018_MplusAutomationPackageFacilitating] and wrote a function (`run_MplusAutomation`) that for each iteration of each condition would save the dataset as a `.dat` file, generate the `.inp` input script for the desired analysis type, and run the model for that dataset. *M*plus then saves the output files (`.out` and `.gh5`). After the analysis, `run_MplusAutomation` extracts parameter estimates and returns them in an `R` object, which, with some additional variables, are saved in separate `.rds` files using wrapper functions described in @sec-pipeline-functions.

```{r, eval = runComponent_Simulation}
<<run_MplusAutomation>>
```

In each analysis, we simulated two MCMC chains (`CHAINS = 2`), and to reduce autocorrelation in the estimated parameters, by defining `THIN = 5` we asked *M*plus to save every 5th sample. By setting `BITERATIONS = 5000(2000)`, we made sure to have between 2000 to 5000 samples (after thinning) for each parameter from each chain. *M*plus considers the first half of each chain as burn-in samples and discards them, thus, in total, we got at least 2000 "independent" samples from both chains combined. Finally, with `FACTORS = ALL (500)` we asked *M*plus to draw 500 samples for each individuals when estimating level-1 parameters. We visually inspected the traceplots and autocorrelation plots of parameter estimates and of a sample of analyzed datasets and good convergence was observed. Furthermore, to make sure the number of iterations and thinning used in the analyses provide sufficiently accurate estimates, we re-analyzed two replications of each alternative DGM with Gaussian and $\chi^2$-distributed means with $N=100$ and $T=100$ with `BITERATIONS = 12500(5000)` and `THIN = 20`, which led to estimates of the parameters of interest (`unstd X.WITH.PHI`, `unstd Variances.PHI`, and `stdyx X.WITH.PHI`) almost identical (up to the third decimal) to those estimated with `BITERATIONS = 5000(2000)` and `THIN = 5` (see below).

The generated input files looked like the following. Note that the `TITLE` and `DATA` strings in the `.inp` files are unique to the dataset being analyzed and included the unique dataset seed `uSeed` (passed to `make_datasets` to generate datasets), the number of individuals in the dataset `N`, the length of the time series `T`, the model types used (`resid.fixed` or `resid.random`), and the replication number `Rep` (see @sec-bookkeeping for further details).

::: panel-tabset
### Fixed residual variance

By estimating the covariances between mean and autoregression (`x phi WITH x phi` under the `BETWEEN%` command), the following *M*plus script runs an AR(1) with random effect mean and autoregressive parameter but with fixed effect residual variance.

```{inp}
#| label: Mplus-input_fixed-resid
#| file: scripts/Mplus-input_fixed-resid.inp
```

### Random residual variance

By estimating the logarithm of the residual variance at level 1 (by `logv | x` under the `%WITHIN%` command) and estimating the covariances between the level-2 mean, autoregression, and residual variance (`x phi logv WITH x phi logv` under the `%BETWEEN%` command), the following *M*plus script runs an AR(1) model with random effect mean, autoregressive parameter, and residual variance.

```{inp}
#| label: Mplus-input_random-resid
#| file: scripts/Mplus-input_random-resid.inp
```

### Comparing different `ANALYSIS` parameters

The table below shows the estimated parameters of two replications of with `BITERATIONS = 5000(2000)` and `THIN = 5` (specified in the table with `2k x 5`) and `BITERATIONS = 12500(5000)` and `THIN = 20` (specified in the table with `5k x 20`). The relevant parameters (level-2 correlations `X.WITH.PHI  unstd`, covariances `X.WITH.PHI stdyx`, and variance `Variances.PHI unstd`) are highlighted with orange.

```{=html}
<iframe width="700" height="500" src="./additional-files/hyperparameters.html" title="Quarto Documentation"></iframe>
```
:::

## The *Harvesting* component {#sec-harvesting-component}

Extracting parameter estimates of individual analyses

The function `fit_extract` gets an `.rds` file generated by `do_fit_doFuture` (see @sec-pipeline-functions)---which includes book-keeping information and Mplus output object that was generated by `run_MplusAutomation` (see @sec-analysis-component)---and extracts the `unstandardized` and `stdyx.standardized` *M*plus parameter estimates and stores them, along with the additional book-keeping information, in a dataframe.

```{r, eval = runComponent_Simulation}
<<fit_extract>>
```

## The *Reporting* component {#sec-reporting-component}

This component includes a function that clean the harvested dataset (and extract the relevant information) and functions to generate dataset profile plots, results figures, and parameter estimates coverage plots.

Given that the figures contain customized typefaces (from the `CMU Serif` and `Merriweather` font families), we need to make load the fonts:

```{r, eval = runComponent_Simulation}
<<report_initialization>>
```

### Cleaning the harvested dataset

Given that the output of `fit_extract` has too much information in it, the function `harvest_cleanup` extracts the relevant parameter estimates, cleans the datasets and makes it *tidy*, and calculates the outcomes of interest (e.g., bias, RMSE, Type-I error rates).

```{r, eval = runComponent_Simulation}
<<harvest_cleanup>>
```

### Making plots of datasets

These functions visualize datasets (simulated or empirical) by generating individual histograms and pairplots of summary statistics of datasets, and a function to generate (and combine) these two plots for any given dataset simulated by `do_sim_parallel` in the pipeline (see @sec-pipeline-functions).

::: panel-tabset
#### Individual histograms

The function `plot_histograms` makes person histograms of the individuals in a dataset (stored as dataframes) and arranges them based on individual means.

```{r, eval = runComponent_Simulation}
<<plot_histograms>>
```

#### Pairplots of summary statistics

The function `plot_pairplots` calculates the mean, variance, and skewness of each individual in a dataset (stored as dataframes) and generates pairplots of their joint distribution.

```{r, eval = runComponent_Simulation}
<<plot_pairplots>>
```

#### Profile plots of simulated datasets

The function `plot_dataset.profile` gets a simulated object (generated by `do_sim_parallel`) and combines individual histograms and pairplots in a single figure.

```{r, eval = runComponent_Simulation}
<<plot_dataset.profile>>
```

We also write the function `save_dataset_profile` to put together two dataset profile plots to get to plots similar to those in Figures S1, S2, S4, and S5 in the Supplemental Materials, and save the final figure as a PDF file.

```{r, eval = runComponent_Simulation}
<<save_dataset_profile>>
```
:::

### Making plots of estimation results

The functions here make the main figures of the paper, which include aspects of model fit (e.g., estimation bias and RMSE, and Type-I error rates, etc.).

::: panel-tabset
#### Plots for a given level-2 distribution

The function `plot_Model.x.Resid` makes a figure with two columns (for the models with fixed or random residual variance), that show the outcomes of a fit measure (e.g., bias) for all the four DGMs with a given level-2 distribution of means for different $N$s and $T$s.

```{r, eval = runComponent_Simulation}
<<plot_Model.x.Resid>>
```

#### Combined plots of results

The function `plot_quadrants` makes a larger figure that includes four *quadrants*, each generated with `plot_Model.x.Resid`., that show the main results of the paper.

```{r, eval = runComponent_Simulation}
<<plot_quadrants>>
```
:::

### Making plots of parameter estimates coverage

The function `plot_caterpillar` makes ...

```{r, eval = runComponent_Simulation}
<<plot_caterpillar>>
```

# Pipeline {#sec-pipeline}

We implemented each of these tasks in separate functions that were essentially wrapper functions (with parallel-computing implementation) around the modular components. Using these wrapper functions, each replication of each simulated condition was saved in a separate `.rds` file. These data files were fed to the analysis wrapper function whose output was saved in separate `.rds` data files. To collect relevant parameter estimates, another wrapper function was used to read the data files and save the desired parameters in a dataframe, which then used in reporting.

## Book-keeping {#sec-bookkeeping}

The outcomes of the components are saved in separate `.rds` files and indexed by unique, descriptive names, and each replication is given a unique numeric identifier that is also used as the random seed used to generate the dataset within each replication. The file names and address are stored in two dataframes along with model parameters used to generate each dataset, and these dataframes are used when reading and writing data files in other components.

::: panel-tabset
### Simulation file references {#sec-bookkeeping-simulation-files}

The function `make_sim_refs` makes a table consisting of all possible combinations of each conditions (that are provided as a list `conditions`), and replicates it `Reps` times (number of replications, $R = 100$). Then, to make sure that each simulated dataset is produced with a unique random seed, the function generates a unique `uSeed` based on the number (and values) of conditions and the replication number. Furthermore, an initial simulation seed (`simSeed`) is included in generation of `uSeed` which makes it possible to have different *batches* of simulations (for instance, if one wants to run the simulation for another 1000 times).

For each condition, a unique seed is generated by weighting different conditions (that are indexed in a vector `d.integer`) by prime numbers (that are not among the prime factors of the number of conditions) and summing them up (which is done with dot-producting `d.integer %*% primes.seq`). Then, to make unique seeds per replications, the replication number is concatenated to the left side of `d.integer %*% primes.seq`, and everything is put into a dataframe (called `d.headers`, that is eventually returned by the function) with unique file names for individual `.rds` files (that include `uSeed` as well as the replication number and the value of condition used in data generation, e.g., `sim_uSeed-13293_l2.dist-Chi2_Model-BinAR_N-100_phi-0.4_T-100_sim.Seed-0_Rep-1`) and the address of the folder in which the `.rds` files were stored. Finally, if desired, the simulation reference table `d.headers` is saved as `.csv` and `.rds`.

```{r, eval = runComponent_Analysis}
<<make_sim_refs>>
```

[***Note that***]{.underline} initially, we started by having two values for the length of the time series ($T = 30$ corresponding to one month of measurements, and $T = 100$ for a reasonably long time series) and one sample size ($N = 100$, and subsample the datasets for $N = 25$ and $N = 50$), and we decided to include five DGMs in the simulations: the AR(1), $\chi^2$AR(1), BinAR(1), PoDAR(1) models, and a DAR(1) model with binomial marginal distribution). Eventually (and midway through the simulations), we decided to omit the $T = 30$ condition and the DAR(1) model from our simulation. However, as omitting these would have led to change in the calculations of `uSeed`, we decided to keep it the simulation reference table intact and omit `Model == "DAR"` and `T == 30` later on (see @sec-pipeline-code for details).

[***Further note that***]{.underline} we only generated datasets with $N = 100$ and $T = 100$, and sub-sampled other sample sizes and time series lengths ($N = 25, 50$ and $T = 25, 50$) from them. Although this had computational benefits (less simulation time, and less storage required), the main motivation behind this decision was to be able to mimic empirical data collection: Sub-sampling the large datasets would be equated with collecting less data than the ideal case in which we have many participants with many measurements (so sub-sampling what *could have been* collected). Furthermore, comparing the datasets with the same DGM and level-2 distribution would be more meaningful: All datasets with same `Rep` value within the same DGM (say, PoDAR(1) model) and with the same level-2 distribution (say, $\chi^2$ distribution) have the same `uSeed`, thus they can be matched. Consequently, the differences we observe for different $N$s and $T$s within each cell of the figures reported in the paper can mimic the effect of "non-ideal" sampling.

Changing the conditions (omitting `Model == "DAR"` and `T == 30` from the simulation reference dataset, and adding additional conditions for `N` and `T`) is done when running the pipeline (see @sec-pipeline-code) and the sub-sampling is implemented when running the analysis in `do_fit_doFuture` (see @sec-pipeline-functions).

### Analysis outputs file references {#sec-bookkeeping-analysis-files}

Another dataframe was generated to include the references of analyses results per simulation condition and analysis condition (or *hyperparameter*s), and file names and directory of individual `.rds` analysis outputs. The hyperparameters included the analysis type (with fixed vs. random residual variance) and the number of iterations and thinning in the MCMC algorithm, which were then used as arguments of `run_MplusAutomation`.

```{r, eval = runComponent_Analysis}
<<make_fit_refs>>
```
:::

## Pipeline functions {#sec-pipeline-functions}

We implemented the components described in @sec-components using the following functions.

::: panel-tabset
### Simulation in parallel

The function `do_sim_parallel` is a wrapper around `make_datasets` (see @sec-dataset-generation) and uses the `clusterApplyLB` function from the package `snow` [@tierney_2021_SnowSimpleNetwork] for parallelization. This wrapper gets the dataframe of simulation file references (`sim_refs`, which is the output of `make_sim_refs`), and for each of its rows, simulates an $N \times T$ dataframe and stores it with other information in the same row---and also additional info about the time the simulating the dataset started and ended---in a list, and saves each list as a separate `.rds` file in the target folder (specified with `sim.Path`). Having the additional data in the saved files allow using them independent from the table of references. Note that to prevent the overload of the hard drive with many parallel write requests (to save the simulated datasets), for the first `nClust` rounds, using the `Sys.sleep` function, the data generation is started after a varying amount of delay. Given that every dataset simulation in the first round may take different amount of times, after the first round is complete, the simulations on different cores fall out of sync and there is no more need for adding delays for the next rounds.

```{r, eval = runComponent_Analysis}
<<do_sim_parallel>>
```

### Analysis in parallel

The function `do_fit_doFuture` is a wrapper around `run_MplusAutomation` (see @sec-analysis-component) and uses the package `doFuture` [@bengtsson_2021_UnifyingFrameworkParallel] as a parallelization backend to `plyr::a_ply`. The function gets a dataframe containing references of analyses results (`fit_refs`, which is the output of `make_fit_refs`), and row by row reads the simulated datasets, sub-samples the data if $N < 100$ or $T < 100$ (see the second note under `make_sim_refs` in @sec-bookkeeping) and runs the analysis based on the hyperparameters specified in `fit_refs`. It then saves the outcome of the analysis (the *M*plus model object produced by `MplusAutomation::mplusModeler` and returned by `run_MplusAutomation`), which includes *M*plus outputs and parameter estimates) with additional information (other values in the same row of `fit_refs` as well as timings) in separate `.rds` files in the target folder. Note that, again, to prevent the overload of the hard drive with many parallel read and write requests (to read the simulated datasets and save the *M*plus outputs), for the first `nClust` rounds, using the `Sys.sleep` function, the data analysis is started after a varying amount of delay. Given that every analysis instance in the first round take different amount of times, after the first round is complete the analyses on cores fall out of sync and there is no more need for adding delays for the next rounds.

```{r, eval = runComponent_Analysis}
<<do_fit_doFuture>>
```

### Harvesting in parallel

The function `do_harvest_doFuture` is a wrapper around `fit_extract` (see @sec-harvesting-component) and uses the package `doFuture` as a parallelization backend to `foreach`. It gets a list of `.rds` files (that can be taken from or `fit_refs`, or by using `list.files` function in the directory in which the analysis results are saved) and returns a large dataframe including parameter estimates (and additional information) extracted by `fit_extract`. Again, like in `do_sim_parallel` and `do_fit_doFuture`, the first `nClust` files are read with varying amount of delay to prevent the overload of the hard drive by multiple read requests.

```{r, eval = runComponent_Analysis}
<<do_harvest_doFuture>>
```
:::

## Pipeline code {#sec-pipeline-code}

The following is the code we used to run the whole pipeline, which consists of three chunk that are responsible of the following:

1.  Creating the book-keeping reference files (using functions described in @sec-bookkeeping).
2.  Running the whole simulation study (using `do_sim_parallel` and `do_fit_doFuture` described in @sec-pipeline-functions). The codes in this chunk can be interrupted (deliberately or otherwise) and resumed by running it again.
3.  Harvesting the parameter estimates and storing them in a single dataframe (using `do_harvest_doFuture` of @sec-pipeline-functions). The resulting dataframe is then used to make the plots using in @sec-simulation-results)

But first we need to set the directories in which the results of each chunk is saved:

```{r, eval = runComponent_Analysis}
#| code-fold: false
<<pipeline_set_directories>>
```

::: panel-tabset
### Making references

The following code makes `sim_refs` and `fit_refs` tables and make backups of them. In line 17 the rows with `T != 100` and `Model == "DAR"` are omitted, and with the codes in lines 22-31 we add other values of $N$s and $T$s (see the notes under `make_sim_refs` in @sec-bookkeeping).

```{r, eval = runComponent_Analysis}
<<pipeline_make_references>>
```

### Running the study

Here we reads the reference tables `sim_refs` and `fit_refs`. We then make a list of already simulated files (that are located in the `sim.Path` directory) and a list of analysis output files (that are located in the `fit.Path` directory). Then, by comparing `sim_refs` and `fit_refs` with the the lists of simulated and analyzed files, we only simulate/analyze the datasets that have not been yet simulated/analyzed. This allows us resume the simulation study after an interruption to the R session.

```{r, eval = runComponent_Analysis}
<<pipeline_run_study>>
```

### Harvesting and cleaning the results

dds

```{r, eval = runComponent_Analysis}
<<pipeline_harvest_results>>
```
:::

## Simulation results {#sec-simulation-results}

The simulation results were

# Additional analyses and figures {#sec-additional-analyses-and-figures}

## Profiles of simulated datasets {#sec-profiles-of-simulated-datasets}

## COGITO data analysis {#sec-cogito-data-analysis}
